<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>MRAI : Home</title>
    <!-- Favicon -->
    <link rel="shortcut icon" type="image/icon" href="assets/images/favicon.ico"/>
    <!-- Font Awesome -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
    <!-- Line icon -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.4.1/css/simple-line-icons.css">
    <!-- Bootstrap -->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Slick slider -->
    <link href="assets/css/slick.css" rel="stylesheet">
    <!-- Gallery Lightbox -->
    <link href="assets/css/magnific-popup.css" rel="stylesheet">
    <!-- Theme color -->
    <link id="switcher" href="assets/css/theme-color/default-theme.css" rel="stylesheet">

    <!-- Main Style -->
    <link href="style.css" rel="stylesheet">

    <!-- Fonts -->

    <!-- Open Sans for body font -->
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,600,700,800" rel="stylesheet">
    <!-- Montserrat for Title -->
  	<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
 
 
	
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>

  	<!-- Start Header -->
	<header id="mu-header" class="" role="banner">
		<div class="container">
			<nav class="navbar navbar-default mu-navbar">
		  <div class="container-fluid">
		    <!-- Brand and toggle get grouped for better mobile display -->
		    <div class="navbar-header">
		      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
		        <span class="sr-only">Toggle navigation</span>
		        <span class="icon-bar"></span>
		        <span class="icon-bar"></span>
		        <span class="icon-bar"></span>
		      </button>

		      <!-- Text Logo -->
		      <!-- <a class="navbar-brand" href="#">MRAI</a> -->

		      <!-- Image Logo -->
		      <a class="navbar-brand" href="index.html"><img src="assets/images/MRAI_white_long.png"></a>

		    </div>

		    <!-- Collect the nav links, forms, and other content for toggling -->
		    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
		      	<ul class="nav navbar-nav mu-menu navbar-right">
					<li><a href="#mu-service">Background</a></li>
					<li><a href="#mu-portfolio">Technical Highlights</a></li>
					<li><a href="#mu-team">Limitations & Future Direction</a></li>
					<li><a href="#mu-contact">Resources</a></li>
			        <li><a href="index.html">Home</a></li>
		      	</ul>
		    </div><!-- /.navbar-collapse -->
		  </div><!-- /.container-fluid -->
		</nav>
		</div>
	</header>
	<!-- End Header -->

	<!-- Start Featured Slider -->

	<section id="mu-featured-slider">
		<div class="row">
			<div class="col-md-12">
				<div class="mu-featured-slide">

					<!-- Start Single slide -->
					<div class="mu-featured-slider-single">
						<img src="assets/images/learn_more_bg.jpg"> 
						<div class="mu-featured-slider-content">
							<h1>MRAI uses deep learning techniques to automatically segment tissues and detect potential pathologies in MRI scans.</h1>
							<p>Artificial Intelligence for Radiologists</p>
							<div class="col-md-12">
								<a href="#mu-service" class="mu-primary-btn">Scroll Down to Learn More</a>
							</div>
						</div>
					</div>
					<!-- End Single slide -->
				</div>
			</div>			
		</div>

		<div class="scroll-down", onclick="scrollToSection('mu-service')">
			<a href="#mu-service"><span></span></a>

			
		</div>
	</section>
	
	<!-- Start Featured Slider -->

		<!-- Start Service -->
		<section id="mu-service">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="mu-service-area">

							<div class="mu-service-header">
								<h2 class="mu-heading-title">Background</h2>
								<span class="mu-header-dot"></span>
								<p style="font-size: 18px;">Current Radiology Workflow, Problem, & User Research</p>
							</div>

							<!-- Start Service Content -->
							<div class="mu-service-content">
								<div class="row">
									<img src="assets/images/radiology_workflow.png" alt="Radiology Workflow">	
									<br><br><br>
									<p style="font-size: 18px;">Current radiology workflow begins with a patient consultation, during which the healthcare provider discusses the need for a knee MRI scan and obtains relevant medical history. After the patient is prepared, the patient is positioned on the MRI table, lying flat on their back with the knee of interest centered in the scanner's field of view. A specialized radiofrequency coil may be positioned around the knee area to enhance image quality during the scan.</p>
									<br>
									<p style="font-size: 18px;">The MRI scanner captures detailed images of the knee's internal structures using magnetic fields and radio waves. One can imagine the patient's knee is sliced layer by layer, and a picture is taken for each layer. After the scan is completed, the acquired images are transferred to a computer system where a radiologist reviews and interprets them. The radiologist assesses the knee's various structures, such as bones, cartilage, ligaments, and tendons.</p>
									<br>
									<p style="font-size: 18px;">The radiologist generates a detailed report containing their findings and diagnoses, which is then communicated to the healthcare provider.</p>
									<br>
									<p style="font-size: 18px;">A typical MRI scan often generates anywhere from 50 to a few hundred image slices. Radiologists manually segment tissues and detect potential diseases by iterating through each of these image slices of a patient. This process is extremely time-intensive and subject to inter- and intra-observer variations such as image artifacts, anatomy differences etc., and hence limits the use of routine MRI use in clinical practice.</p>
									<br>
									<p style="font-size: 18px;">Our team conducted interviews with four radiologists, and gained valuable insights into their workflows, pain points, and perspectives on AI-assisted radiology product. Key takeaways are as follows:</p>
									<ul>
										<li>Some radiologists embrace AI/ML, whereas others are wary of the integration of AI/ML in their work;</li>
										<li>Completely automate the MRI scan interpretation can be challenging;</li>
										<li>Product aim should be to accelerate MR image analysis;</li>
										<li>Product feature suggestions include providing overlay toggle function for tissue segmentation, providing summary information and avoid information overload, and patient info anonymization.</li>
									</ul>
								</div>
							</div>
							<!-- End Service Content -->

						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- End Service -->

		<!-- Start Portfolio -->
		<section id="mu-portfolio">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="mu-portfolio-area">

							<div class="mu-portfolio-header">
								<h2 class="mu-heading-title">Technical <span>Highlights</span></h2>
								<span class="mu-header-dot"></span>
								
							</div>

							<!-- Start Portfolio Content -->
							<div class="mu-service-content">
								<div class="row">
									<div class="col-md-6">
										<h3>Dataset</h3>
										<br>
										<p style="font-size: 18px; font-style: italic;"><a href="https://aimi.stanford.edu/shared-datasets/" target="_blank" style="color: #4568DC;">SKM-TEA: Stanford Knee MRI Multi-Task Evaluation Dataset</a> 
										</p>
										<p style="font-size: 18px;">
											<ul>
												<li>A collection of quantitative knee MRI scans acquired clinically at Stanford Healthcare</li>
												<li>155 patients received a knee MRI with the qDESS sequence on one of two 3 Tesla (3T) GE MR750 scanners</li>
												<li>Each patient's scan contains 160 knee images, which allows radiologists to view the patient's knee, a 3d object, as a series of 2d images</li>
												<li>Training: 86 scans; Validation: 33 scans; Test: 36 scans</li>
												<li>4 Tissue labels for segmentation: patellar cartilage, femoral cartilage, meniscus, tibial cartilage</li>
												<li>2 Pathology labels derived from annotations: abnormal (cartilage lesion, ligament tear, meniscal tear, effusion) and normal (no abnormality)</li>
											</ul>
										</p>
									</div>
									<div class="col-md-6">
										<img src="assets/images/output.gif" alt="knee img gif">	
										<br><br><br>
									</div>
									
									<div class="col-md-12">
										<br><br>
										<h3>Model Pipeline</h3>										
										<br>
										<img src="assets/images/model_pipeline.gif" alt="model pipeline">
										<br><br><br>
										
										<h3>Segmentation Model: U-Net</h3>
									</div>
									<br><br>
									<div class="col-md-6">
										<img src="assets/images/u-net-architecture.png" alt="unet architecture">
									</div>
									<div class="col-md-6">
										<p style="font-size: 18px;">U-Net is the most widespread image segmentation architecture due to its flexibility, optimized modular design, and success in all medical image modalities.
										</p>
										<p style="font-size: 18px;">It is able to handle high-resolution images and produce accurate pixel-level segmentation maps. Its architecture is compact, suitable for real-time or near real-time applications in healthcare.
										</p>
									</div>

									<div class="col-md-12">
									<br>
									<h5>Segmentation Model Performance</h5>
									<p style="font-size: 18px;">The pixel-level accuracy for each of the four tissues is around 90% (<b>Table 1</b>). The model is generally able to predict tissue labels accurately when the tissues are present and when there is no tissue (<b>Fig. 1</b>). There are cases where the model struggles to predict correctly when there is tissue present (<b>Fig. 2a</b>), and there are instances of over-prediction (<b>Fig. 2b</b>). Further improvement is needed to address these issues.
									</p>
									<br>
									</div>

									<div class="col-md-6">
										<table style="width: 90%">
											<caption style="caption-side: top;">Table 1: Pixel-Level Accuracy Per Tissue Label</caption>
											<tr>
												<th>Label</th>
												<th style="text-align: center">Pixel-Level Accuracy</th>
											</tr>
											<tr>
												<td>patellar cartilage</td>
												<td style="text-align: center">93.6%</td>
											</tr>
											<tr>
												<td>femoral cartilage</td>
												<td style="text-align: center">90.7%</td>
											</tr>
											<tr>
												<td>medial/lateral meniscus</td>
												<td style="text-align: center">89.3%</td>
											</tr>
											<tr>
												<td>medial/lateral tibial cartilage</td>
												<td style="text-align: center">86.8%</td>
											</tr>
										  </table>
										  <br><br>
									</div>

									<div class="col-md-6">
										<img src="assets/images/seg_cm.png" alt="tissue segmentation model confusion matrix">
										<figcaption><b>Figure 1: </b>Confusion matrix of tissue segmentation model. The tissue segmentation model is able to predict tissue labels accurately when the tissues are present (48%) and when there is no tissue (47.6%).</figcaption>
										<br><br>
									</div>

									<div class="col-md-12">
										<img src="assets/images/seg_ex_1.png" alt="tissue segmentation wrong prediction example 1">
										<figcaption><b>Figure 2a: </b>Example of wrong prediction of the tissue segmentation model. The model missed significant portions of the meniscus tissue in the predicted mask (arrow pointed region).</figcaption>
										<br><br>
									</div>

									<div class="col-md-12">
										<img src="assets/images/seg_ex_2.png" alt="tissue segmentation wrong prediction example 2">
										<figcaption><b>Figure 2b: </b>Example of wrong prediction of the tissue segmentation model. The model over-predicted areas of patellar cartilage and femoral cartilage (arrow pointed region).</figcaption>
									</div>
										
									<div class="col-md-12">
										<br><br><br>
										<h3>Pathology Detection Model: EfficientNet</h3>
										<br>										
										<div class="col-md-6">
											<img src="assets/images/EfficientNet.png" alt="EfficientNet architecture">
										</div>
										<div class="col-md-6">
											<p style="font-size: 18px;">EfficientNet uses compound scaling to improve performance with increased computational efficiency.
											</p>
											<p style="font-size: 18px;">It is capable of a wide range of image classification tasks, achieving state-of-the-art accuracy for benchmark dataset, with an order-of-magnitude fewer parameters, and hence great for transfer learning.
											</p>
										</div>
									</div>

									<div class="col-md-12">
										<br>
										<h5>Pathology Detection Model Performance</h5>
										<p style="font-size: 18px;">The pathology detection model's performance is measured using balanced accuracy, to take into consideration of the dataset's class imbalance issue. It achieved an 87% balanced accuracy and an F2 score of 87%, indicating a good overall performance. Our focus is on minimizing false negatives to avoid missed diagnose and hence the use of F2 score.
										</p>
										<p style="font-size: 18px;">The model generally performs well in predicting abnormalities when there are lesions or tears on the scan, as well as correctly identifying normal scans with no lesions (<b>Fig. 3</b>). However, it makes incorrect predictions in very rare cases, such as failing to detect a lesion (<b>Fig. 4a</b>) and misidentifying a normal scan as abnormal (<b>Fig. 4b</b>). To improve accuracy, the plan is to fine-tune the model and include additional features like contour and texture.
										</p>
										<br>
									</div>
	
									<div class="col-md-6">
										<br>
										<table style="width: 90%">
											<caption style="caption-side: top;">Table 2: Pathology Detection Model Performance</caption>
											<tr>
												<th style="text-align: left">Metric</th>
												<th style="text-align: center">Value</th>
											</tr>
											<tr>
												<td style="text-align: left">Balanced Accuracy</td>
												<td style="text-align: center">87.43%</td>
											</tr>
											<tr>
												<td style="text-align: left">F2 Score</td>
												<td style="text-align: center">87.27%</td>
											</tr>
										</table>
										<br><br>
									</div>
										
									<div class="col-md-6">
										<img src="assets/images/pathology_cm.png" alt="pathology detection model confusion matrix">
										<figcaption><b>Figure 3: </b>Confusion matrix of pathology detection model. The pathology detection model is able to predict abnormalities when there are lesions or tears on the scan (46.5%) and correctly identifying normal scans with no lesions (40.6%).</figcaption>
										<br><br>
									</div>
									<div class="col-md-6">
										<img src="assets/images/pathology_ex_1.png" alt="pathology detection wrong prediction example 1">
										<figcaption><b>Figure 4a: </b>Example of wrong prediction of the pathology detection model. The model is unable to detect a lesion (arrow pointed region).</figcaption>
										<br><br>
									</div>
									<div class="col-md-6">
										<img src="assets/images/pathology_ex_2.png" alt="pathology detection model confusion matrix">
										<figcaption><b>Figure 4b: </b>Example of wrong prediction of the pathology detection model. The model is misidentifying a normal scan as abnormal.</figcaption>
									</div>

									<div class="col-md-12">
										<br><br>
										<h3>End-to-End AWS Architecture</h3>
										<br>
										<img src="assets/images/aws_architecture.png" alt="aws architecture">
										<br><br><br>
										<p style="font-size: 18px;">Our MVP is entirely deployed on AWS Infrastructure.</p>
										<p style="font-size: 18px;">Patient's scan in HDF5 format can be uploaded by user from User Interface (UI). The UI is deployed on an AWS EC2 instance, and the Apache Web Server is used to serve the UI files and handle HTTP requests. All services interact using fast API endpoints. Once the patent's scan is uploaded, a unique identifier is generated for the file. The unique identifier serves two purposes:
											<ul>
												<li>Using the unique identifier, user can retrieve tissue segmentation and pathology summary without re-executing the model within 30 days</li>
												<li>The unique identifier allows different users to upload files with the same name and generate predictions without overriding existing data</li>
											</ul>
										</p>
										<p style="font-size: 18px;">Amazon Sagemaker Notebook instance is used for model training. Sagemaker is also used for deploying models as endpoints for inference. To make the architecture scalable, Lambda is used, because of its ability to handle millions of request, to create URL endpoints to trigger the model endpoints.
										</p>
										<p style="font-size: 18px;">Amazon S3 is used for persisting uploaded patient's scans, various JSON files created at different stages of processing and model inference.
										</p>
									</div>
								</div>
							</div>
							<!-- End Portfolio Content -->				

						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- End Portfolio -->

		<!-- Start Team -->
		<section id="mu-team">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="mu-team-area">

							<div class="mu-team-header">
								<h2 class="mu-heading-title">Limitations & <span>Future Direction</span></h2>
								<span class="mu-header-dot"></span>
							</div>

							<!-- Start Team Content -->
							<div class="mu-team-content">
								<div class="col-md-6">
									<h3 style="text-align: center;">Limitations</h3>
									<br>
									<ol style="font-size: 18px;">
										<li>The knee images are generated from two different GE scanners, hence limiting the ability to validate models on images generated from other vendor’s scanners</li>
										<br>
										<li>Models are trained on the SKM-TEA dataset, which only contains knee scans of sagittal plane from qDESS GE scanners and belongs to a specific subgroup of the general population based on geographic, demographic, and health insurance constraints</li>
										<br>
										<li>Only echo1 images are used for training and inference</li>
										<br>
										<li>Currently the pathology detection model can only classify if abnormalities exist</li>
										<br><br>
									</ol>
								</div>
								<div class="col-md-6">
									<h3 style="text-align: center;">Ethical & Data Privacy Considerations</h3>
									<br>
									<ol style="font-size: 18px;">
										<li>The SKM-TEA dataset is publicly distributed under the Stanford University School of Medicine license</li>
										<br>
										<li>Patient protected health information (PHI) metadata was anonymized from DICOM files, which were all subsequently manually inspected</li>
										<br>
										<li>Raw data, sensitivity maps, and scanner-generated and SENSE reconstructions were manually checked for PHI and quality</li>
										<br>
										<li>MVP will refrain user to enter any PII metadata from User Interface</li>
										<br><br>
									</ol>
								</div>
								<div class="col-md-12">
									<h3 style="text-align: center;">Future Direction</h3>
									<br>
									<p style="font-size: 18px;">Our future roadmap involves more exploration with variability in dataset, enhancing model and UI features, and also integration with scanner provider solutions. First, we need more diversified patient’s data and images from different scanner providers for model training and fine tuning. Second, for tissue segmentation, we would like to expand and segment more granular tissue types. For pathology detection, we would explore multi-label classification with confidence level. We would also introduce reinforcement learning to intake radiologists' input on generated outcome to improve model. Third, we would like to introduce User Interface features such as zoom in, user authentication, DICOM anonymization utility, allowing radiologists to provide feedback, and building interactive reports. Lastly, we would like to work with MR scanner providers to integrate this product with their system.
									</p>
								</div>
							</div>
							<!-- End Team Content -->
						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- End Team -->	
		
		<!-- Start Contact -->
		<section id="mu-contact">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="mu-contact-area">

							<div class="mu-contact-header">
								<h2 class="mu-heading-title">Resources</h2>
								<span class="mu-header-dot"></span>
							</div>

							<!-- Start Contact Content -->
							<div class="mu-contact-content">
								<div class="col-md-12">
									<h5 style="text-align: center;">Relevant Radiology Terminologies</h5>
									<br>
									<ol style="font-size: 18px;">
										<li><b>MRI:</b> Magnetic Resonance Imaging (MRI) is a non-invasive medical imaging technique that uses strong magnetic fields and radio waves to generate detailed images of the internal structures of the body.</li>
										<br>
										<li><b>DICOM (Digital Imaging and Communications in Medicine):</b>  DICOM is the international standard for medical images and related information. It defines the formats for medical images that can be exchanged with the data and quality necessary for clinical use.</li>
										<br>
										<div class="flex">
											<iframe width="560" height="315" src="https://www.youtube.com/embed/jPfMRwR3A-U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
										</div>
										<br>
										<li><b>HDF5 (Hierarchical Data Format Version 5):</b> HDF5 (H5, or .h5) is a generic scientific data format with supporting software. An HDF5 file can contain almost any collection of data entities in a single file, it has become the format of choice for organizing heterogeneous collections consisting of very large and complex datasets.</li>
										<br>
										<li><b>Echo:</b> Echo refers to the signal produced by the MRI machine as a result of the interaction between the magnetic fields and the hydrogen nuclei in the body tissues. </li>
										<br>
										<li><b>Slice:</b> Slice refers to a 2D cross-sectional image that is obtained from a three-dimensional volume of the patient's knee.</li>
										<br>
										<li><b>Sagittal Plane:</b> One of the three primary anatomical planes used in anatomy and medical imaging to describe the orientation and position of structures in the human body. The other two planes are the coronal (frontal) plane and the transverse (horizontal) plane. The sagittal plane is a vertical plane which passes through the body longitudinally. It divides the body into a left section and a right section. A special sagittal plane is the median sagittal plane, which passes down the midline of the body, separating it into equal halves.
										</li>
										<br>
										<li><b>qDESS (Quantitative Double Echo Steady State):</b> qDESS is an MRI pulse sequence technique used in medical imaging. It is a modification of the traditional Double Echo Steady State (DESS) sequence, which utilizes two echoes to create images with improved contrast and signal-to-noise ratio. qDESS is often used in musculoskeletal imaging to assess cartilage and joint health. It can provide quantitative measurements of T1 (longitudinal relaxation time) and T2 (transverse relaxation time) relaxation times, which are valuable for characterizing tissues and detecting abnormalities.</li>
										<br>
										<li><b>PACS (Picture Archiving and Communication System):</b> PACS is a medical imaging technology that provides storage, retrieval, distribution, and presentation of medical images like X-rays, CT scans, MRIs, and other diagnostic images. It is widely used in healthcare settings to streamline the process of managing and accessing medical images and is an essential component of modern radiology departments. PACS systems offer several advantages over traditional film-based systems, including faster image retrieval, remote access to images, reduced physical storage space, and improved collaboration between healthcare professionals.</li>
										<br>
										<li><b>Basics of a Knee MRI:</b>
											<div class="flex">
												<iframe width="560" height="315" src="https://www.youtube.com/embed/gQ2Q8CRtfmQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
											</div>
										</li>
										<br>
										</li>
									</ol>
								</div>
							</div>
							<!-- End Contact Content -->

						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- End Contact -->
		

	</main>
	
	<!-- End main content -->	
			
			
	<!-- Start footer -->
	<footer id="mu-footer" role="contentinfo">
			<div class="container">
				<div class="mu-footer-area">
					<p>&copy; Copyright Team MRAI. All rights reserved. August 2023.</p>
				</div>
			</div>

	</footer>
	<!-- End footer -->

	
	
    <!-- jQuery library -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <!-- Bootstrap -->
    <script src="assets/js/bootstrap.min.js"></script>
	<!-- Slick slider -->
    <script type="text/javascript" src="assets/js/slick.min.js"></script>
    <!-- Filterable Gallery js -->
    <script type="text/javascript" src="assets/js/jquery.filterizr.min.js"></script>
    <!-- Gallery Lightbox -->
    <script type="text/javascript" src="assets/js/jquery.magnific-popup.min.js"></script>
    <!-- Ajax contact form  -->
    <script type="text/javascript" src="assets/js/app.js"></script>
    
	
    <!-- Custom js -->
	<script type="text/javascript" src="assets/js/custom.js"></script>
	<script>
		function scrollToSection(sectionID) {
            var section = document.getElementById(sectionID);
            if (section) {
                var sectionPosition = section.getBoundingClientRect().top;
                var offsetPosition = window.pageYOffset + sectionPosition;

                window.scrollTo({
                    top: offsetPosition,
                    behavior: "smooth"
                });
            }
        }
	</script>



  </body>
</html>